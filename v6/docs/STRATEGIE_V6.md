# StratÃ©gie Architecture V6 - Assistant Gazelle
**Date:** 28 dÃ©cembre 2025
**Status:** Draft Initial
**Objectif:** Architecture industrielle 100% fiable, maintenable, extensible

---

## ğŸ¯ Philosophie: De "Bricolage" Ã  "SystÃ¨me Industriel"

### ProblÃ¨mes Actuels (V5)
1. **Logique mÃ©langÃ©e** - Fetch + Transform + Load dans un seul fichier
2. **Erreurs silencieuses** - FK violations bloquent tout, difficile Ã  debugger
3. **Sync "tout ou rien"** - FenÃªtre temporelle arbitraire (15/30 jours)
4. **Pas d'observabilitÃ©** - print() statements, aucun dashboard
5. **Couplage fort** - Impossible de changer une partie sans tout casser

### Vision V6
> **"Chaque composant fait UNE chose, et la fait parfaitement"**

---

## ğŸ—ï¸ Architecture: Les 4 Piliers

### Pilier 1: EXTRACTEUR ISOLÃ‰ (The Fetcher)
**ResponsabilitÃ©:** Parler Ã  Gazelle, rien d'autre

```
gazelle-fetcher/
â”œâ”€â”€ fetchers/
â”‚   â”œâ”€â”€ client_fetcher.py
â”‚   â”œâ”€â”€ piano_fetcher.py
â”‚   â”œâ”€â”€ timeline_fetcher.py
â”‚   â””â”€â”€ user_fetcher.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ graphql_client.py      # GraphQL pur
â”‚   â”œâ”€â”€ rate_limiter.py        # Gestion API limits
â”‚   â””â”€â”€ token_manager.py       # OAuth refresh
â””â”€â”€ output/
    â””â”€â”€ raw_data/              # JSON brut, aucune transformation
```

**CaractÃ©ristiques:**
- âœ… **Aucune dÃ©pendance Supabase** - Ne sait mÃªme pas que Supabase existe
- âœ… **Validation minimaliste** - "Gazelle a rÃ©pondu? OK, on sauvegarde"
- âœ… **Reprise sur Ã©chec** - Checkpoints automatiques (page 47/100 Ã©chouÃ©e? On reprend Ã  47)
- âœ… **Format standardisÃ©** - Toujours du JSON avec metadata (timestamp, version API, etc.)

**Tests:**
```bash
# Le fetcher doit pouvoir tourner SEUL
python -m fetchers.timeline_fetcher --output raw_data/timeline.json
# RÃ©sultat: 1 fichier JSON, rien dans Supabase
```

---

### Pilier 2: ZONE DE TRANSIT (Staging Area)

**ResponsabilitÃ©:** Accepter TOUT, jamais rejeter

```sql
-- Tables Staging (prÃ©fixe stg_)
CREATE TABLE stg_timeline_entries (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    raw_data JSONB NOT NULL,              -- DonnÃ©es brutes Gazelle
    fetched_at TIMESTAMPTZ DEFAULT NOW(),
    processed BOOLEAN DEFAULT FALSE,
    processing_attempts INT DEFAULT 0,
    last_error TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes pour performance
CREATE INDEX idx_stg_timeline_processed ON stg_timeline_entries(processed);
CREATE INDEX idx_stg_timeline_fetched ON stg_timeline_entries(fetched_at DESC);
```

**RÃ¨gles:**
- âœ… **Accepte mÃªme les donnÃ©es invalides** - On log, on n'arrÃªte jamais
- âœ… **Idempotent** - MÃªme entrÃ©e 10 fois = 1 seul record
- âœ… **TraÃ§abilitÃ© totale** - On sait quand, d'oÃ¹, combien de tentatives
- âœ… **Purge automatique** - Les donnÃ©es > 30 jours processed=true sont archivÃ©es

---

### Pilier 3: MOTEUR DE RÃ‰CONCILIATION (The Matcher)

**ResponsabilitÃ©:** Transformer Staging â†’ Production sans jamais Ã©chouer

```
reconciler/
â”œâ”€â”€ matchers/
â”‚   â”œâ”€â”€ user_matcher.py        # usr_XXX â†’ users.id
â”‚   â”œâ”€â”€ client_matcher.py      # cln_XXX â†’ clients.id
â”‚   â””â”€â”€ piano_matcher.py       # pno_XXX â†’ pianos.id
â”œâ”€â”€ transformers/
â”‚   â”œâ”€â”€ timeline_transformer.py
â”‚   â””â”€â”€ measurement_transformer.py
â”œâ”€â”€ rules/
â”‚   â”œâ”€â”€ validation_rules.py
â”‚   â””â”€â”€ business_rules.py
â””â”€â”€ fallbacks/
    â””â”€â”€ orphan_handler.py      # Que faire si user inconnu?
```

**Logique de RÃ©conciliation (Exemple: Users):**
```python
def reconcile_user(gazelle_user_id: str) -> str:
    """
    Retourne TOUJOURS un user_id valide, jamais None.
    """
    # 1. Chercher dans users table
    user = db.query("SELECT id FROM users WHERE id = ?", gazelle_user_id)
    if user:
        return user.id

    # 2. CrÃ©er un placeholder
    placeholder = db.insert("users", {
        'id': gazelle_user_id,
        'first_name': 'Inconnu',
        'last_name': f'({gazelle_user_id})',
        'is_placeholder': True  # Flag pour admin
    })

    # 3. Notifier admin
    notify_admin(f"Nouveau technicien dÃ©tectÃ©: {gazelle_user_id}")

    return placeholder.id
```

**RÃ©sultat:** Aucune FK violation possible, l'import ne peut jamais Ã©chouer.

---

### Pilier 4: OBSERVABILITÃ‰ (Monitoring)

**ResponsabilitÃ©:** RÃ©pondre aux 3 questions critiques

```sql
CREATE TABLE sync_status (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    sync_type TEXT NOT NULL,              -- 'timeline', 'clients', etc.
    started_at TIMESTAMPTZ NOT NULL,
    completed_at TIMESTAMPTZ,
    status TEXT NOT NULL,                 -- 'running', 'success', 'failed'

    -- MÃ©triques
    records_fetched INT DEFAULT 0,
    records_processed INT DEFAULT 0,
    records_skipped INT DEFAULT 0,
    records_failed INT DEFAULT 0,

    -- SantÃ©
    error_rate FLOAT,                     -- % d'erreurs
    data_freshness_score FLOAT,           -- 0-100%

    -- Debug
    error_summary JSONB,
    performance_metrics JSONB,

    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Dashboard query
SELECT
    sync_type,
    MAX(completed_at) as last_success,
    COUNT(*) FILTER (WHERE status='failed') as failures_24h,
    AVG(data_freshness_score) as avg_freshness
FROM sync_status
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY sync_type;
```

**Dashboard Simple (API Endpoint):**
```json
GET /api/v6/health

{
  "timeline": {
    "last_sync": "2025-12-28T15:30:00Z",
    "freshness": 98.5,
    "status": "healthy",
    "records_pending": 3
  },
  "clients": {
    "last_sync": "2025-12-28T12:00:00Z",
    "freshness": 100,
    "status": "healthy",
    "records_pending": 0
  }
}
```

---

## ğŸ”„ Pipeline de DonnÃ©es V6

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 1: EXTRACTION                                        â”‚
â”‚  gazelle-fetcher â†’ raw_data/*.json                          â”‚
â”‚  â€¢ Aucune transformation                                    â”‚
â”‚  â€¢ Rate limiting automatique                                â”‚
â”‚  â€¢ Reprise sur Ã©chec                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 2: STAGING                                           â”‚
â”‚  raw_data/*.json â†’ stg_* tables                             â”‚
â”‚  â€¢ Accepte tout (mÃªme invalide)                             â”‚
â”‚  â€¢ DÃ©tection de doublons                                    â”‚
â”‚  â€¢ Flag processed=false                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 3: RÃ‰CONCILIATION                                    â”‚
â”‚  stg_* â†’ production tables (clients, pianos, etc.)          â”‚
â”‚  â€¢ Matching intelligent (fuzzy, levenshtein)                â”‚
â”‚  â€¢ CrÃ©ation de placeholders si besoin                       â”‚
â”‚  â€¢ Jamais d'Ã©chec (fallback toujours)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‰TAPE 4: MONITORING                                        â”‚
â”‚  sync_status table + dashboard                              â”‚
â”‚  â€¢ MÃ©triques temps rÃ©el                                     â”‚
â”‚  â€¢ Alertes automatiques (Slack, Email)                      â”‚
â”‚  â€¢ Rapports quotidiens                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Tables de Production (SchÃ©ma UnifiÃ©)

### Principes de Design
1. **IDs Gazelle partout** - Fini les UUIDs, on utilise les IDs natifs de Gazelle
2. **Soft deletes** - Jamais de DELETE, toujours `deleted_at`
3. **Audit trail** - Toutes les tables ont `created_at`, `updated_at`, `synced_at`
4. **Versioning** - Champ `version` pour dÃ©tecter les changements

```sql
-- Exemple: Table users (techniciens)
CREATE TABLE users (
    id TEXT PRIMARY KEY,                  -- usr_ofYggsCDt2JAVeNP
    external_id TEXT,
    first_name TEXT,
    last_name TEXT,
    email TEXT,
    phone TEXT,
    role TEXT,
    is_placeholder BOOLEAN DEFAULT FALSE, -- CrÃ©Ã© par rÃ©conciliation?

    -- Audit
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    synced_at TIMESTAMPTZ,                -- DerniÃ¨re sync Gazelle
    deleted_at TIMESTAMPTZ,               -- Soft delete

    -- MÃ©tadata
    version INT DEFAULT 1,
    raw_data JSONB                        -- Backup des donnÃ©es Gazelle brutes
);

-- Trigger auto-update
CREATE TRIGGER update_users_updated_at
    BEFORE UPDATE ON users
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
```

---

## ğŸš€ StratÃ©gie de Synchronisation

### Mode IncrÃ©mental (Par DÃ©faut)
```python
# Au lieu de "fenÃªtre glissante de 30 jours"
last_sync = get_last_successful_sync('timeline')
entries = fetcher.get_timeline_entries(since=last_sync)

# Avantages:
# âœ… Charge minimale API
# âœ… Sync rapide (secondes au lieu de minutes)
# âœ… Temps rÃ©el possible (toutes les 5 minutes)
```

### Mode Full Sync (Rare, PlanifiÃ©)
```python
# 1x par semaine, la nuit
entries = fetcher.get_timeline_entries(full=True)

# Use case:
# - DÃ©tecter suppressions Gazelle
# - Corriger inconsistances
# - Audit de santÃ©
```

### Mode RÃ©paration (Manuel)
```python
# Admin trigger pour ressync une pÃ©riode spÃ©cifique
entries = fetcher.get_timeline_entries(
    start='2025-12-01',
    end='2025-12-31'
)
```

---

## ğŸ§© ExtensibilitÃ©: Ajouter un Nouveau Volet

**Exemple: Ajouter module "Facturation"**

### Ã‰tape 1: CrÃ©er le Fetcher (5 minutes)
```python
# v6/gazelle-fetcher/fetchers/invoice_fetcher.py
class InvoiceFetcher(BaseFetcher):
    def fetch(self):
        query = "query { allInvoices { ... } }"
        return self.graphql_client.query(query)
```

### Ã‰tape 2: CrÃ©er Table Staging (2 minutes)
```sql
CREATE TABLE stg_invoices (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    raw_data JSONB NOT NULL,
    processed BOOLEAN DEFAULT FALSE
);
```

### Ã‰tape 3: CrÃ©er Table Production (10 minutes)
```sql
CREATE TABLE invoices (
    id TEXT PRIMARY KEY,  -- inv_XXX
    client_id TEXT REFERENCES clients(id),
    amount DECIMAL(10,2),
    -- ... autres champs
);
```

### Ã‰tape 4: CrÃ©er Reconciler (20 minutes)
```python
# v6/reconciler/transformers/invoice_transformer.py
def transform(raw_invoice):
    return {
        'id': raw_invoice['id'],
        'client_id': match_client(raw_invoice['client']),
        'amount': raw_invoice['total']
    }
```

### Ã‰tape 5: Ajouter au Pipeline (2 minutes)
```python
# v6/pipeline/main.py
SYNC_MODULES = [
    'clients',
    'pianos',
    'timeline',
    'invoices',  # â† Nouveau module
]
```

**Total: ~40 minutes pour ajouter un volet complet**

---

## ğŸ›¡ï¸ Gestion des Erreurs (Zero Trust)

### Principe: "Jamais faire confiance, toujours vÃ©rifier"

```python
class SafeReconciler:
    def process_entry(self, raw_data):
        try:
            # Validation
            if not self.validate(raw_data):
                self.log_and_skip(raw_data, "Invalid schema")
                return

            # Transformation
            transformed = self.transform(raw_data)

            # Matching
            matched = self.match_references(transformed)

            # Insertion (avec retry)
            self.insert_with_retry(matched, max_retries=3)

        except ValidationError as e:
            # Erreur attendue: on skip, on log
            self.mark_as_skipped(raw_data, str(e))

        except DatabaseError as e:
            # Erreur DB: on retry plus tard
            self.mark_for_retry(raw_data, str(e))

        except Exception as e:
            # Erreur inattendue: on alerte
            self.alert_admin(raw_data, str(e))
            self.mark_as_failed(raw_data, str(e))
```

**RÃ©sultat:** Une erreur ne bloque JAMAIS tout le pipeline.

---

## ğŸ“ˆ MÃ©triques de SuccÃ¨s

### KPIs V6
1. **Uptime:** 99.9% des syncs rÃ©ussissent
2. **Latence:** DonnÃ©es Gazelle â†’ Supabase < 5 minutes
3. **FraÃ®cheur:** 98%+ des donnÃ©es < 1h de retard
4. **FiabilitÃ©:** 0 perte de donnÃ©es
5. **ObservabilitÃ©:** Tout problÃ¨me dÃ©tectable en < 30 secondes

### Dashboard Admin
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Assistant Gazelle V6 - SantÃ© SystÃ¨me      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Timeline Entries    âœ“ 98.5%  (5 min ago)  â”‚
â”‚  Clients             âœ“ 100%   (1h ago)     â”‚
â”‚  Pianos              âœ“ 99.2%  (10 min ago) â”‚
â”‚  Users               âš  95.1%  (2h ago)     â”‚
â”‚                                             â”‚
â”‚  ğŸ“Š DerniÃ¨res 24h:                          â”‚
â”‚    â€¢ 1,847 entrÃ©es synchronisÃ©es           â”‚
â”‚    â€¢ 3 erreurs (0.16%)                     â”‚
â”‚    â€¢ 2 nouveaux techniciens dÃ©tectÃ©s       â”‚
â”‚                                             â”‚
â”‚  âš ï¸ Actions requises:                       â”‚
â”‚    â€¢ VÃ©rifier placeholder: usr_ABC123      â”‚
â”‚    â€¢ RÃ©soudre 3 timeline orphelines        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Stack Technologique

### Backend
- **Python 3.11+** (async/await pour performance)
- **FastAPI** (API V6 moderne)
- **SQLAlchemy 2.0** (ORM avec type safety)
- **Pydantic V2** (Validation donnÃ©es)
- **Celery + Redis** (Tasks async, scheduling)

### Base de DonnÃ©es
- **Supabase PostgreSQL** (Production)
- **Triggers PostgreSQL** (Auto-updates, audit)
- **Partitioning** (Timeline entries par mois)

### Monitoring
- **Sentry** (Error tracking)
- **Prometheus + Grafana** (MÃ©triques)
- **OU Simple: Table sync_status** (Commencer petit)

### CI/CD
- **GitHub Actions** (Tests automatiques)
- **Docker** (DÃ©ploiement consistant)
- **Pre-commit hooks** (Quality gates)

---

## ğŸ“… Plan de Migration V5 â†’ V6

### Phase 1: Fondations (Semaine 1)
- [x] CrÃ©er structure v6/
- [ ] Migrer table users (FAIT dans V5)
- [ ] CrÃ©er tables staging (stg_*)
- [ ] CrÃ©er table sync_status
- [ ] Ã‰crire BaseFetcher (classe abstraite)

### Phase 2: Premier Module (Semaine 2)
- [ ] Timeline Fetcher complet
- [ ] Timeline Reconciler
- [ ] Tests end-to-end timeline
- [ ] Migration donnÃ©es V5 â†’ V6

### Phase 3: Modules Restants (Semaine 3-4)
- [ ] Clients, Pianos, Users
- [ ] Dashboard monitoring basique
- [ ] Documentation API

### Phase 4: Production (Semaine 5)
- [ ] Tests de charge
- [ ] DÃ©ploiement staging
- [ ] Migration finale
- [ ] Rollback plan

---

## ğŸ“ LeÃ§ons Apprises (V5)

### âŒ Ce qui n'a PAS marchÃ©
1. **FenÃªtres temporelles fixes** (15/30 jours) - Arbitraire et inefficace
2. **FK violations bloquantes** - 1 user manquant = tout casse
3. **Logs print()** - Impossible de debug en production
4. **Logique monolithique** - Sync + Transform dans 1 fichier de 800 lignes
5. **Pas de tests** - Chaque changement = roulette russe

### âœ… Ce qui a BIEN marchÃ©
1. **Gazelle IDs natifs** - Plus simple que UUIDs
2. **GraphQL pagination** - GÃ¨re bien les gros volumes
3. **Supabase RLS** - SÃ©curitÃ© native
4. **Structure modulaire** (core/, modules/) - Bonne base

### ğŸ¯ Principes V6 (Non NÃ©gociables)
1. **SÃ©paration stricte des responsabilitÃ©s**
2. **Fail gracefully, jamais tout casser**
3. **Observable depuis le jour 1**
4. **Tests automatiques obligatoires**
5. **Documentation = Code (pas un PDF Word)**

---

## ğŸš¦ CritÃ¨res de SuccÃ¨s V6

Avant de dire "V6 est prÃªte", on doit pouvoir rÃ©pondre OUI Ã :

- [ ] Pipeline peut tourner 1000x sans supervision?
- [ ] Une erreur dans Timeline n'affecte pas Clients?
- [ ] Admin sait en < 1 minute si sync a Ã©chouÃ©?
- [ ] Ajouter un nouveau module prend < 1 jour?
- [ ] DonnÃ©es fraÃ®ches en < 5 minutes aprÃ¨s changement Gazelle?
- [ ] 0 perte de donnÃ©es mÃªme si Supabase down 1h?
- [ ] Nouveau dev comprend l'architecture en < 30 minutes?

---

## ğŸ“– Prochaines Ã‰tapes

1. **Valider cette stratÃ©gie** avec l'Ã©quipe
2. **CrÃ©er POC** (Timeline Fetcher + Staging + Reconciler)
3. **Mesurer performance** (1000 entries en combien de temps?)
4. **ItÃ©rer** sur base de rÃ©sultats rÃ©els

---

**Document vivant** - Mis Ã  jour au fur et Ã  mesure de l'implÃ©mentation.
