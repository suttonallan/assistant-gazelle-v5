# üî¨ Caract√©risation des Imports Timeline v4 vs v5

**Date:** 2026-01-18
**Source:** Analyse comparative Windows (C:\Genosa\Working) vs Mac (assistant-gazelle-v5)

---

## üìã Table des Mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Syst√®me v4 (Windows - C:\Genosa\Working)](#syst√®me-v4-windows)
3. [Syst√®me v5 (Mac - assistant-gazelle-v5)](#syst√®me-v5-mac)
4. [Comparaison D√©taill√©e](#comparaison-d√©taill√©e)
5. [Recommandations pour v6](#recommandations-pour-v6)

---

## Vue d'ensemble

### Architecture v4 (Windows)
- **Plateforme:** Windows
- **Base de donn√©es:** SQL Server local
- **API:** GraphQL Gazelle (priv√©e)
- **Mode d'import:** Historique complet + mises √† jour quotidiennes
- **Scripts principaux:** `Import_all_data.py`, `Import_daily_update.py`, `timeline.py`

### Architecture v5 (Mac)
- **Plateforme:** macOS
- **Base de donn√©es:** Supabase (PostgreSQL cloud)
- **API:** GraphQL Gazelle (priv√©e) avec OAuth2
- **Mode d'import:** Fen√™tre glissante 7 jours + backfills historiques
- **Scripts principaux:** `sync_to_supabase.py`, `history_recovery_year_by_year.py`

---

## Syst√®me v4 (Windows)

### üìÇ Structure des Scripts

#### 1. `Import_all_data.py` - Import Historique Complet

**Fen√™tre temporelle:**
```python
EVENTS_END_DATE = datetime.now() + timedelta(days=365)   # +1 an futur
EVENTS_START_DATE = datetime.now() - timedelta(days=10*365)  # -10 ans pass√©
```

**Caract√©ristiques:**
- ‚úÖ R√©cup√®re **10 ans d'historique**
- ‚úÖ Inclut **1 an dans le futur** (rendez-vous planifi√©s)
- ‚úÖ Utilise les filtres GraphQL `startOn` et `endOn`
- ‚è±Ô∏è **Dur√©e:** Longue (~plusieurs heures pour import complet)
- üéØ **Usage:** Import initial ou r√©initialisation compl√®te

**Filtres GraphQL:**
```python
initial_filters = {
    "startOn": EVENTS_START_DATE.strftime('%Y-%m-%d'),
    "endOn": EVENTS_END_DATE.strftime('%Y-%m-%d'),
    "type": ["APPOINTMENT", "PERSONAL", "MEMO", "SYNCED"]
}
```

---

#### 2. `Import_daily_update.py` - Mises √† Jour Quotidiennes

**Fen√™tre temporelle:**
```python
EVENTS_END_DATE = datetime.now() + timedelta(days=90)   # +90 jours futur
EVENTS_START_DATE = datetime.now() - timedelta(days=60)  # -60 jours pass√©
```

**Caract√©ristiques:**
- ‚úÖ R√©cup√®re **60 jours pass√©s + 90 jours futurs** (150 jours total)
- ‚úÖ Capture les modifications r√©centes et rendez-vous √† venir
- ‚è±Ô∏è **Dur√©e:** Rapide (~quelques minutes)
- üéØ **Usage:** Synchronisation quotidienne automatique
- üîÑ **Fr√©quence:** 1x par jour (cron/scheduler)

**Rationnelle:**
- 60 jours pass√©s = capture corrections/notes tardives des techniciens
- 90 jours futurs = capture planification 3 mois √† l'avance

---

#### 3. `timeline.py` - Timeline Entries

**Configuration:**
```python
# Set to 0 for a full re-import of all timeline entries.
# Set to any other number (e.g., 365) for a faster incremental sync.
LOOKBACK_DAYS = 365
```

**Caract√©ristiques:**
- ‚úÖ **Mode 1:** `LOOKBACK_DAYS = 0` ‚Üí Import complet (toutes les entr√©es)
- ‚úÖ **Mode 2:** `LOOKBACK_DAYS = 365` ‚Üí Derni√®re ann√©e seulement
- üéØ **Flexibilit√©:** Param√®tre configurable selon besoin
- ‚è±Ô∏è **Dur√©e:** Variable (0 = plusieurs heures, 365 = ~30 min)

**Usage typique:**
- Import initial: `LOOKBACK_DAYS = 0`
- Syncs quotidiennes: `LOOKBACK_DAYS = 365`

---

### üîë Points Cl√©s v4

1. **Strat√©gie Double:**
   - Import complet (`Import_all_data.py`) ‚Üí Utilis√© rarement (setup initial)
   - Import incr√©mental (`Import_daily_update.py`) ‚Üí Utilis√© quotidiennement

2. **Fen√™tres Temporelles:**
   - **Historique complet:** 10 ans pass√© + 1 an futur = **11 ans total**
   - **Quotidien:** 60 jours pass√© + 90 jours futur = **150 jours total**

3. **Filtres GraphQL:**
   - Utilise `startOn` et `endOn` pour limiter les donn√©es r√©cup√©r√©es
   - Types filtr√©s: `APPOINTMENT`, `PERSONAL`, `MEMO`, `SYNCED`

4. **Base de donn√©es:**
   - SQL Server local (Windows)
   - Pas de contraintes de bande passante cloud

---

## Syst√®me v5 (Mac)

### üìÇ Structure des Scripts

#### 1. `sync_to_supabase.py::sync_timeline_entries()` - Sync Quotidienne

**Fen√™tre temporelle:**
```python
# Date de cutoff: 7 jours en arri√®re (fen√™tre glissante)
now = datetime.now()
cutoff_date = now - timedelta(days=7)  # -7 jours uniquement
```

**Caract√©ristiques:**
- ‚úÖ **Fen√™tre glissante 7 jours** (optimisation 2026-01-11)
- ‚úÖ Utilise filtre API `occurredAtGet` (>= cutoff)
- ‚úÖ **UPSERT** avec `on_conflict=external_id` (anti-doublons)
- ‚è±Ô∏è **Dur√©e:** <30 secondes (~100-500 entr√©es)
- üéØ **Usage:** Synchronisation automatique (scheduler 01:00 AM)

**Requ√™te GraphQL:**
```graphql
query GetTimelineEntries($cursor: String, $occurredAtGet: CoreDateTime) {
    allTimelineEntries(first: 100, after: $cursor, occurredAtGet: $occurredAtGet) {
        edges {
            node {
                id
                occurredAt
                type
                summary
                comment
                client { id }
                piano { id }
                invoice { id }
                estimate { id }
                user { id }
            }
        }
        pageInfo {
            hasNextPage
            endCursor
        }
    }
}
```

**Rationnelle 7 jours:**
- Base historique d√©j√† dans Supabase
- Capture notes r√©centes de Margot et techniciens
- Corrections de la semaine incluses
- Performance optimis√©e (20x plus rapide que historique complet)
- Protection: Si sync √©choue, le lendemain rattrape automatiquement

---

#### 2. `history_recovery_year_by_year.py` - Backfill Historique

**Fen√™tre temporelle:**
```python
# Import ann√©e par ann√©e (ex: 2024 ‚Üí 2016)
start_date = f"{year}-01-01T00:00:00Z"
end_date = f"{year}-12-31T23:59:59Z"
```

**Caract√©ristiques:**
- ‚úÖ Import **ann√©e par ann√©e** (strat√©gie robuste)
- ‚úÖ Batch de **500 entr√©es** par insertion
- ‚úÖ **Gestion d'erreurs isol√©es** (continue si batch √©choue)
- ‚úÖ Retry entr√©e par entr√©e si batch √©choue (FK manquantes)
- ‚è±Ô∏è **Dur√©e:** Variable (d√©pend du nombre d'ann√©es)
- üéØ **Usage:** R√©cup√©ration historique one-time

**Process:**
1. R√©cup√®re toutes les entr√©es depuis le d√©but de l'ann√©e (`since_date`)
2. Filtre pour garder uniquement l'ann√©e cible
3. Insert par batch de 500 avec UPSERT (`on_conflict=external_id`)
4. Si batch √©choue, retry entr√©e par entr√©e avec `user_id=NULL`

**Mapping des types:**
```python
type_mapping = {
    'APPOINTMENT': 'APPOINTMENT',
    'CONTACT_EMAIL': 'CONTACT_EMAIL',
    'CONTACT_EMAIL_AUTOMATED': 'CONTACT_EMAIL',
    'SERVICE_ENTRY_AUTOMATED': 'SERVICE_ENTRY_MANUAL',
    'SYSTEM_MESSAGE': 'SYSTEM_NOTIFICATION',
    'INVOICE': 'INVOICE_PAYMENT',
    'service': 'SERVICE_ENTRY_MANUAL',
    # ... d√©faut: 'NOTE'
}
```

**Extraction de mesures:**
```python
# Extrait humidit√© (%), temp√©rature (¬∞), fr√©quence (Hz) du texte
humidity_match = re.search(r'(\d+)\s*%', text)
temp_match = re.search(r'(\d+)\s*¬∞', text)
freq_match = re.search(r'(\d+)\s*Hz', text, re.IGNORECASE)
```

---

#### 3. `core/gazelle_api_client.py::get_timeline_entries()` - API Client

**M√©thode:**
```python
def get_timeline_entries(
    self,
    limit: Optional[int] = None,
    since_date: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    R√©cup√®re les entr√©es de timeline avec pagination automatique.

    Args:
        limit: Nombre max d'entr√©es (None = toutes)
        since_date: Date ISO depuis laquelle r√©cup√©rer (filtre occurredAtGet)
    """
```

**Caract√©ristiques:**
- ‚úÖ **Pagination automatique** (cursor-based)
- ‚úÖ R√©cup√®re 100 entr√©es par page (`first: 100`)
- ‚úÖ Filtre optionnel `occurredAtGet` (>= date)
- ‚úÖ Continue jusqu'√† `hasNextPage = false`
- üîÑ **Retry automatique** sur erreurs r√©seau

**Champs r√©cup√©r√©s:**
- `id` (external_id dans Supabase)
- `occurredAt` (CoreDateTime UTC)
- `type` (APPOINTMENT, SERVICE_ENTRY_MANUAL, etc.)
- `summary` (title dans Supabase)
- `comment` (description dans Supabase)
- `client { id }`, `piano { id }`, `user { id }`
- `invoice { id }`, `estimate { id }`

---

### üîë Points Cl√©s v5

1. **Strat√©gie Hybride:**
   - Sync quotidienne: **7 jours glissants** (performance optimis√©e)
   - Backfill historique: **Ann√©e par ann√©e** (robustesse)

2. **Optimisations:**
   - UPSERT avec `on_conflict=external_id` ‚Üí Aucun doublon
   - Fen√™tre 7 jours ‚Üí 20x plus rapide que v4 quotidien
   - Pagination automatique ‚Üí G√®re volumes importants

3. **Base de donn√©es:**
   - Supabase (PostgreSQL cloud)
   - Contraintes de bande passante ‚Üí Optimisation critique

4. **Gestion d'erreurs:**
   - Retry par entr√©e si batch √©choue
   - Mapping flexible des types (fallback `NOTE`)
   - Extraction automatique de mesures depuis texte

---

## Comparaison D√©taill√©e

### üìä Fen√™tres Temporelles

| Aspect | v4 Windows | v5 Mac |
|--------|------------|--------|
| **Import Complet** | 10 ans pass√© + 1 an futur<br/>(11 ans total) | Ann√©e par ann√©e<br/>(flexible) |
| **Sync Quotidienne** | 60 jours pass√© + 90 jours futur<br/>(150 jours total) | 7 jours pass√©<br/>(7 jours total) |
| **Performance Quotidienne** | ~5-10 minutes | <30 secondes |
| **Gain v5 vs v4** | - | **20x plus rapide** |

### üéØ Strat√©gies d'Import

#### v4 Windows - Approche "Large Filet"
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Import Quotidien: -60 jours ‚Üí +90 jours   ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê   ‚îÇ
‚îÇ  [‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ]   ‚îÇ
‚îÇ        60j pass√©   NOW    90j futur         ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  Volume: ~10,000-50,000 entr√©es/jour        ‚îÇ
‚îÇ  Dur√©e: 5-10 minutes                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Avantages:**
- ‚úÖ Capture exhaustive (3 mois de donn√©es)
- ‚úÖ Rattrape corrections tardives (60 jours)
- ‚úÖ Planification long terme (90 jours futurs)

**Inconv√©nients:**
- ‚ùå Volume √©lev√© quotidiennement
- ‚ùå Dur√©e d'ex√©cution longue
- ‚ùå Bande passante importante

---

#### v5 Mac - Approche "Fen√™tre Glissante"
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Sync Quotidienne: -7 jours seulement       ‚îÇ
‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê             ‚îÇ
‚îÇ         [‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè]                          ‚îÇ
‚îÇ          7j   NOW                           ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  Volume: ~100-500 entr√©es/jour              ‚îÇ
‚îÇ  Dur√©e: <30 secondes                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Avantages:**
- ‚úÖ Performance ultra-rapide (20x)
- ‚úÖ Bande passante minimale
- ‚úÖ Capture notes r√©centes (Margot)
- ‚úÖ Protection: Rattrapage automatique si √©chec

**Inconv√©nients:**
- ‚ö†Ô∏è N√©cessite historique pr√©-charg√©
- ‚ö†Ô∏è Moins de marge pour corrections tardives

**Solution v5:**
- Historique pr√©-charg√© via `history_recovery_year_by_year.py`
- Backfills ponctuels si n√©cessaire

---

### üîÑ Gestion des Doublons

| Aspect | v4 Windows | v5 Mac |
|--------|------------|--------|
| **M√©thode** | MERGE SQL Server | UPSERT Supabase |
| **Cl√© unique** | `id` (probablement) | `external_id` |
| **Comportement** | INSERT ou UPDATE | `on_conflict=external_id` |
| **Garantie** | D√©pend config SQL | Math√©matique (constraint unique) |

**v5 Protection anti-doublons:**
```python
# UPSERT avec on_conflict sur external_id (cl√© unique Gazelle)
url = f"{supabase_url}/gazelle_timeline_entries?on_conflict=external_id"
headers["Prefer"] = "resolution=merge-duplicates"

# Comportement:
# Sync 1: INSERT entry_123 ‚Üí ‚úÖ Cr√©√©
# Sync 2: UPSERT entry_123 ‚Üí ‚úÖ MAJ (pas de doublon)
# Sync 3: UPSERT entry_123 ‚Üí ‚úÖ MAJ (toujours pas de doublon)
```

---

### üìù Mapping des Donn√©es

#### v4 Windows - Filtres de Types
```python
# Filtres explicites dans la requ√™te
initial_filters = {
    "type": ["APPOINTMENT", "PERSONAL", "MEMO", "SYNCED"]
}
```

#### v5 Mac - Mapping Flexible
```python
# Accepte tous les types, mappe vers sch√©ma Supabase
type_mapping = {
    'CONTACT_EMAIL_AUTOMATED': 'CONTACT_EMAIL',
    'SERVICE_ENTRY_AUTOMATED': 'SERVICE_ENTRY_MANUAL',
    'INVOICE': 'INVOICE_PAYMENT',
    'service': 'SERVICE_ENTRY_MANUAL',
    # ... d√©faut: 'NOTE'
}
```

**Avantage v5:**
- ‚úÖ Plus flexible (accepte types inconnus)
- ‚úÖ Fallback automatique (`NOTE`)
- ‚úÖ Extraction automatique de mesures (%, ¬∞, Hz)

---

### üõ†Ô∏è Extraction de Mesures

| Aspect | v4 Windows | v5 Mac |
|--------|------------|--------|
| **Temp√©rature** | ‚ùì (non document√©) | ‚úÖ Regex: `(\d+)\s*¬∞` |
| **Humidit√©** | ‚ùì (non document√©) | ‚úÖ Regex: `(\d+)\s*%` |
| **Fr√©quence** | ‚ùì (non document√©) | ‚úÖ Regex: `(\d+)\s*Hz` |
| **Stockage** | ‚ùì | `metadata` (JSONB) |

**v5 Exemple:**
```python
# Texte: "Piano accord√©. 23¬∞, 47%, 440Hz"
measurements = extract_measurements(comment)
# ‚Üí {
#     "temperature": 23.0,
#     "humidity": 47.0,
#     "frequency": 440.0
# }
```

---

### ‚öôÔ∏è Gestion d'Erreurs

#### v4 Windows
- ‚ùì Non document√© (probablement retry global)

#### v5 Mac
```python
# Strat√©gie multi-niveaux:
# 1. Tentative batch de 500
try:
    supabase.upsert(batch, on_conflict='external_id')
except:
    # 2. Retry entr√©e par entr√©e
    for record in batch:
        try:
            # 3. Fallback: user_id=NULL si FK manquante
            safe_record = record.copy()
            safe_record['user_id'] = None
            supabase.upsert(safe_record)
        except:
            stats['errors'] += 1
```

**Avantages v5:**
- ‚úÖ Isolation d'erreurs (une entr√©e cass√©e ne bloque pas le batch)
- ‚úÖ Fallback automatique (FK manquantes)
- ‚úÖ Stats d√©taill√©es (success/errors/batches)

---

## Recommandations pour v6

### üéØ Ce qui Fonctionne Bien

#### De v4 (√† conserver):
1. **Fen√™tre 60 jours pass√© pour corrections tardives**
   - Les techniciens ajoutent parfois notes 2-4 semaines apr√®s service
   - Recommandation v6: **Augmenter de 7 jours ‚Üí 30 jours**

2. **Planification future (90 jours)**
   - Important pour dashboard rendez-vous
   - Recommandation v6: **Ajouter filtre futur si n√©cessaire**

3. **Double strat√©gie (complet + incr√©mental)**
   - Import initial + syncs quotidiennes
   - v5 fait d√©j√† √ßa (backfill + 7 jours)

#### De v5 (√† conserver):
1. **UPSERT avec on_conflict** ‚Üí Anti-doublons math√©matique
2. **Extraction automatique mesures** ‚Üí Enrichissement donn√©es
3. **Mapping flexible types** ‚Üí Robustesse
4. **Gestion erreurs multi-niveaux** ‚Üí Fiabilit√©

---

### üìà Am√©liorations Propos√©es v6

#### 1. **Fen√™tre Temporelle Optimale**

**Proposition:**
```python
# Sync quotidienne v6 (compromis v4/v5)
cutoff_date_past = now - timedelta(days=30)     # 30 jours pass√© (vs 7 v5, 60 v4)
cutoff_date_future = now + timedelta(days=90)   # 90 jours futur (comme v4)

# Rationnelle:
# - 30 jours pass√©: Capture corrections tardives (compromis 7/60)
# - 90 jours futur: Capture planification (comme v4)
# - Volume: ~1,000-3,000 entr√©es (vs 100-500 v5, 10,000+ v4)
# - Dur√©e estim√©e: 1-2 minutes (vs 30s v5, 5-10min v4)
```

**Avantages:**
- ‚úÖ Meilleur √©quilibre performance/exhaustivit√©
- ‚úÖ Capture corrections tardives (30 jours vs 7)
- ‚úÖ Planification rendez-vous (90 jours futurs)
- ‚úÖ Toujours 5x plus rapide que v4

---

#### 2. **Enrichissement avec PrivatePianoMeasurement**

**v5 d√©j√† impl√©ment√©:**
```python
def _enrich_timeline_with_measurements(self):
    """
    v6: Enrichit timeline avec mesures de PrivatePianoMeasurement.

    Strat√©gie:
    1. Pour chaque piano r√©cent, interroger allPianoMeasurements
    2. Si mesure existe dans PrivatePianoMeasurement ‚Üí prioritaire
    3. Sinon, garder extraction texte (metadata)
    """
```

**Recommandation v6:**
- ‚úÖ Conserver cette logique
- ‚úÖ Ajouter logging pour tra√ßabilit√©
- ‚úÖ Stocker source mesure (`text_extraction` vs `piano_measurement`)

---

#### 3. **Mode Hybride Configurable**

**Proposition:**
```python
# Config dynamique selon contexte
SYNC_MODE = os.getenv("TIMELINE_SYNC_MODE", "balanced")  # fast|balanced|exhaustive

sync_configs = {
    "fast": {
        "days_past": 7,
        "days_future": 30,
        "description": "Mode rapide (<30s) - Notes r√©centes uniquement"
    },
    "balanced": {
        "days_past": 30,
        "days_future": 90,
        "description": "Mode √©quilibr√© (1-2min) - Corrections + planification"
    },
    "exhaustive": {
        "days_past": 60,
        "days_future": 365,
        "description": "Mode exhaustif (5-10min) - Maximum de donn√©es"
    }
}
```

**Usage:**
- Sync quotidienne: `SYNC_MODE=balanced` (d√©faut)
- Apr√®s maintenance: `SYNC_MODE=exhaustive` (ponctuel)
- Debug rapide: `SYNC_MODE=fast`

---

#### 4. **Monitoring et Alertes**

**Proposition:**
```python
# Logs d√©taill√©s dans sync_logs
{
    "script_name": "sync_timeline_v6",
    "sync_mode": "balanced",
    "window_config": {
        "days_past": 30,
        "days_future": 90,
        "cutoff_past": "2026-01-01T00:00:00Z",
        "cutoff_future": "2026-04-01T00:00:00Z"
    },
    "stats": {
        "fetched": 2500,
        "success": 2485,
        "errors": 15,
        "measurements_extracted": 342,
        "measurements_enriched": 89
    },
    "execution_time_seconds": 87,
    "status": "success"
}
```

**Alertes:**
- ‚ö†Ô∏è Si `execution_time_seconds` > seuil (mode d√©pendant)
- ‚ö†Ô∏è Si `errors` > 5% du total
- ‚ö†Ô∏è Si `fetched` = 0 (probl√®me API)

---

### üìã Checklist Migration v4 ‚Üí v6

#### Phase 1: Analyse (FAIT ‚úÖ)
- [x] Documenter strat√©gie v4 Windows
- [x] Documenter strat√©gie v5 Mac
- [x] Identifier forces/faiblesses
- [x] Proposer optimisations v6

#### Phase 2: Impl√©mentation
- [ ] Impl√©menter mode `balanced` (30j pass√© + 90j futur)
- [ ] Configurer `SYNC_MODE` dans `.env`
- [ ] Ajouter logging d√©taill√© (window_config, stats)
- [ ] Impl√©menter alertes (execution_time, errors)

#### Phase 3: Tests
- [ ] Tester mode `fast` (7j pass√©, 30j futur)
- [ ] Tester mode `balanced` (30j pass√©, 90j futur)
- [ ] Tester mode `exhaustive` (60j pass√©, 365j futur)
- [ ] V√©rifier performance (dur√©e, volume)
- [ ] V√©rifier qualit√© (aucun doublon, mesures extraites)

#### Phase 4: Validation
- [ ] Comparer volume v4 vs v6 (mode balanced)
- [ ] V√©rifier corrections tardives captur√©es (30j pass√©)
- [ ] V√©rifier rendez-vous futurs (90j futur)
- [ ] Valider extraction mesures vs PrivatePianoMeasurement

---

## üéì Le√ßons Apprises

### De v4 Windows:
1. **Large fen√™tre = S√©curit√© mais co√ªt performance**
   - 60 jours pass√© + 90 jours futur = exhaustif
   - Mais: 5-10 minutes quotidiennement

2. **Planification future importante**
   - Dashboard rendez-vous n√©cessite +90 jours
   - Ne pas optimiser au point de perdre cette vue

3. **Corrections tardives r√©elles**
   - Techniciens ajoutent notes 2-4 semaines apr√®s
   - 7 jours v5 probablement trop court

### De v5 Mac:
1. **UPSERT = S√©curit√© anti-doublons**
   - Constraint unique + on_conflict = math√©matique
   - Permet syncs multiples sans risque

2. **Extraction automatique = Enrichissement**
   - Regex sur texte ‚Üí mesures structur√©es
   - Valorise donn√©es existantes

3. **Gestion erreurs multi-niveaux = Robustesse**
   - Batch ‚Üí Entr√©e individuelle ‚Üí Fallback FK
   - Maximise taux de succ√®s

### Pour v6:
1. **Compromis > Extr√™mes**
   - Ni 7 jours (trop court) ni 60 jours (trop long)
   - 30 jours = sweet spot

2. **Configuration > Hard-code**
   - Modes (fast/balanced/exhaustive)
   - Adaptable selon contexte

3. **Monitoring > Espoir**
   - Logs d√©taill√©s + alertes
   - D√©tection proactive probl√®mes

---

## üìö R√©f√©rences

### Documents v5 Mac:
- [RECAP_FINAL_IMPORTS.md](../RECAP_FINAL_IMPORTS.md) - Optimisation fen√™tre 7 jours
- [VALIDATION_IMPORTS_NUIT.md](../docs/VALIDATION_IMPORTS_NUIT.md) - Validation strat√©gie v5
- [CSV_TO_SUPABASE_MAPPING.md](../CSV_TO_SUPABASE_MAPPING.md) - Mapping CSV ‚Üí Supabase

### Scripts v5 Mac:
- [modules/sync_gazelle/sync_to_supabase.py](../modules/sync_gazelle/sync_to_supabase.py) - Sync quotidienne 7j
- [scripts/history_recovery_year_by_year.py](../scripts/history_recovery_year_by_year.py) - Backfill ann√©e par ann√©e
- [core/gazelle_api_client.py](../core/gazelle_api_client.py) - Client API GraphQL

### Scripts v4 Windows (C:\Genosa\Working):
- `Import_all_data.py` - Import complet 10 ans
- `Import_daily_update.py` - Sync quotidienne 60j pass√© + 90j futur
- `timeline.py` - Timeline avec LOOKBACK_DAYS configurable

---

**Document cr√©√© le:** 2026-01-18
**Par:** Assistant Claude Code + Allan Sutton
**Statut:** ‚úÖ ANALYSE COMPL√àTE - PR√äT POUR v6
